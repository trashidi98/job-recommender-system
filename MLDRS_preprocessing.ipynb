{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zesha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zesha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zesha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import freeze_support\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from gensim.models import Phrases, CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newStopWords = ['u','www','com','ca','xa0','b','age', 'color', 'national', 'origin', 'citizenship', 'physical', 'mental', 'disability', 'race', 'religion', 'creed', 'gender', 'sex', 'sexual', 'orientation', 'gender', 'identity', 'expression', 'genetic', 'marital','veteran']\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING\n",
    "\n",
    "def remove_punc(corpus):\n",
    "    punc_free = \"\".join([i for i in corpus if i not in string.punctuation])\n",
    "    return punc_free\n",
    "\n",
    "def tokenization(corpus):\n",
    "    tokens = wpt.tokenize(corpus)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    output = [i for i in corpus if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "def lemmization(corpus):\n",
    "    lemm = [wordnet_lemmatizer.lemmatize(word) for word in corpus]\n",
    "    return lemm\n",
    "\n",
    "def cleanResume(corpus):\n",
    "    corpus = re.sub('httpS+s*', ' ', corpus)  # remove URLs\n",
    "    corpus = re.sub('RT|cc', ' ', corpus)  # remove RT and cc\n",
    "    corpus = re.sub('#S+', '', corpus)  # remove hashtags\n",
    "    corpus = re.sub('@S+', '  ', corpus)  # remove mentions\n",
    "    corpus = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~•’\"\"\"), ' ', corpus)  # remove punctuations\n",
    "    return corpus\n",
    "\n",
    "def preprocessing(resumes):\n",
    "    resumes['cleaned'] = resumes.apply(lambda x: cleanResume(x))\n",
    "    print(\"Cleaned corpus\")\n",
    "    resumes['cleaned'] = resumes['cleaned'].apply(lambda x:remove_punc(x))\n",
    "    print(\"Removed punctuation\")\n",
    "    resumes['cleaned'] = resumes['cleaned'].apply(lambda x:x.lower())\n",
    "    print(\"Lowercase\")\n",
    "    resumes['cleaned'] = resumes['cleaned'].apply(lambda x:tokenization(x))\n",
    "    print(\"Tokenized corpus\")\n",
    "    resumes['cleaned'] = resumes['cleaned'].apply(lambda x:remove_stopwords(x))\n",
    "    print(\"Removed Stopwords\")\n",
    "    resumes['cleaned'] = resumes['cleaned'].apply(lambda x:lemmization(x))\n",
    "    print(\"Lemmatized corpus\")\n",
    "    resumes.head()\n",
    "    return resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizeLemm(corpus_list):\n",
    "\n",
    "    #Tokenise the corpus\n",
    "    tokenized_corp = [word_tokenize(i) for i in corpus_list]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(tokenized_corp)\n",
    "\n",
    "    #Remove words that don't feature 20 times and those that feature in over 50% of documents\n",
    "    id2word.filter_extremes(no_below=20, no_above=0.5, keep_n=80000)\n",
    "\n",
    "    texts = tokenized_corp\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus_final = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return corpus_final, id2word, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
